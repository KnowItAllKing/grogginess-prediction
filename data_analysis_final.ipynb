{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_54370/3113811900.py:3: FutureWarning: In a future version of pandas, parsing datetimes with mixed time zones will raise an error unless `utc=True`. Please specify `utc=True` to opt in to the new behaviour and silence this warning. To create a `Series` with mixed offsets and `object` dtype, please use `apply` and `datetime.datetime.strptime`\n",
      "  data['date'] = pd.to_datetime(data['ISO8601'], errors='coerce')\n"
     ]
    }
   ],
   "source": [
    "person = 'Luc Rieffel'\n",
    "data = pd.read_csv(f'{person}/autosleep-{person}.csv')\n",
    "data['date'] = pd.to_datetime(data['ISO8601'], errors='coerce')\n",
    "data['date'] = data['date'].apply(lambda x: x.tz_localize(None).normalize() if x is not pd.NaT else pd.NaT)\n",
    "data['year'] = data['date'].dt.year\n",
    "data['month'] = data['date'].dt.month\n",
    "data['day'] = data['date'].dt.day\n",
    "data['inBed_minutes'] = pd.to_timedelta(data['inBed']).dt.total_seconds() / 60\n",
    "data['day_of_week'] = data['date'].dt.dayofweek\n",
    "data['is_weekend'] = data['day_of_week'].apply(lambda x: 1 if x >= 5 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_columns(df):\n",
    "  column_mappings = {\n",
    "      'Name': 'name',\n",
    "      'Date': 'date',\n",
    "      'Grogginess': 'grogginess',\n",
    "      'How many alarms did you set?': 'num_alarms',\n",
    "      'What time did you set your first alarm for?': 'first_alarm',\n",
    "      'Did you take sleep-aiding medicine (not weed, ie melatonin/antihystamine)?': 'sleep_medicine',\n",
    "      \"What was the temperature when you woke up? (Don't include Â°F, just the number)\": 'waking_temp',\n",
    "      'Were you intoxicated when you went to sleep?': 'intoxicated',\n",
    "      'Were you sick when you went to sleep?': 'sick',\n",
    "      'Did you eat within an hour of going to bed?': 'eat_before_bed',\n",
    "      'Did you sleep alone?': 'sleep_alone',\n",
    "      'Did you sleep in your own bed/room?': 'own_bed',\n",
    "      'How stressed were you last night?': 'stress',\n",
    "      'Did you use your phone before going to sleep?': 'phone_before_bed',\n",
    "      \"When was the latest you ingested caffeine before going to bed? (Don't answer if N/A)\": 'caffeine_before_bed'\n",
    "  }\n",
    "  \n",
    "  df_renamed = df.rename(columns=column_mappings)\n",
    "  return df_renamed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "suppl = rename_columns(pd.read_csv('form.csv'))\n",
    "\n",
    "suppl['date'] = pd.to_datetime(suppl['date'], format='%m/%d/%Y').dt.normalize()\n",
    "\n",
    "\n",
    "def time_to_minutes(time_str):\n",
    "  if pd.isna(time_str) or time_str == '':\n",
    "      return 0\n",
    "  time = pd.to_datetime(time_str, format='%I:%M:%S %p')\n",
    "  return time.hour * 60 + time.minute\n",
    "\n",
    "suppl['first_alarm'] = suppl['first_alarm'].apply(time_to_minutes)\n",
    "suppl['caffeine_before_bed'] = suppl['caffeine_before_bed'].apply(time_to_minutes)\n",
    "lag_periods = 1\n",
    "suppl['grogginess_lag1'] = suppl['grogginess'].shift(lag_periods)\n",
    "suppl = suppl[suppl['name'] == person].fillna(0)\n",
    "# suppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58\n"
     ]
    }
   ],
   "source": [
    "merged_data = pd.merge(suppl, data, on='date', how='inner').fillna(0)\n",
    "\n",
    "\n",
    "column_headers = list(merged_data.columns.values)\n",
    "\n",
    "print(len(merged_data))\n",
    "\n",
    "merged_data.to_csv(f'{person}/merged_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Timestamp         name       date  grogginess  num_alarms  \\\n",
      "0   2/20/2024 8:32:10  Luc Rieffel 2024-02-20           3        0.25   \n",
      "1  2/21/2024 11:25:14  Luc Rieffel 2024-02-21           5        0.75   \n",
      "2  2/22/2024 11:05:02  Luc Rieffel 2024-02-22           8        0.75   \n",
      "3  2/23/2024 11:03:56  Luc Rieffel 2024-02-23           7        0.00   \n",
      "4  2/24/2024 13:19:27  Luc Rieffel 2024-02-24           8        0.25   \n",
      "5  2/25/2024 13:37:49  Luc Rieffel 2024-02-25           4        0.00   \n",
      "6  2/26/2024 11:05:02  Luc Rieffel 2024-02-26           4        0.75   \n",
      "7  2/27/2024 11:07:32  Luc Rieffel 2024-02-27           1        0.75   \n",
      "8  2/28/2024 11:15:24  Luc Rieffel 2024-02-28           3        0.75   \n",
      "9  2/29/2024 11:38:59  Luc Rieffel 2024-02-29           4        0.75   \n",
      "\n",
      "   first_alarm  sleep_medicine  waking_temp  intoxicated  eat_before_bed  ...  \\\n",
      "0     0.816901             1.0     1.000000          0.0             0.0  ...   \n",
      "1     0.816901             1.0     1.000000          0.0             0.0  ...   \n",
      "2     0.816901             1.0     1.000000          0.0             0.0  ...   \n",
      "3     0.000000             1.0     1.000000          0.0             0.0  ...   \n",
      "4     0.929577             1.0     0.985294          0.0             0.0  ...   \n",
      "5     0.000000             0.0     1.000000          0.0             0.0  ...   \n",
      "6     0.816901             1.0     0.985294          0.0             0.0  ...   \n",
      "7     0.816901             1.0     1.000000          0.0             0.0  ...   \n",
      "8     0.816901             1.0     0.970588          0.0             0.0  ...   \n",
      "9     0.816901             0.0     0.985294          0.0             0.0  ...   \n",
      "\n",
      "    respMin   respMax  tags  notes  year month day inBed_minutes day_of_week  \\\n",
      "0  0.787879  0.564516   0.0    0.0  2024     2  20      0.599732    0.166667   \n",
      "1  0.787879  0.661290   0.0    0.0  2024     2  21      0.583668    0.333333   \n",
      "2  0.757576  0.951613   0.0    0.0  2024     2  22      0.729585    0.500000   \n",
      "3  0.787879  0.645161   0.0    0.0  2024     2  23      0.637216    0.666667   \n",
      "4  0.818182  0.741935   0.0    0.0  2024     2  24      0.520750    0.833333   \n",
      "5  0.848485  1.000000   0.0    0.0  2024     2  25      0.887550    1.000000   \n",
      "6  0.787879  1.000000   0.0    0.0  2024     2  26      0.781794    0.000000   \n",
      "7  0.787879  0.951613   0.0    0.0  2024     2  27      0.475234    0.166667   \n",
      "8  0.787879  0.645161   0.0    0.0  2024     2  28      0.645248    0.333333   \n",
      "9  0.757576  0.612903   0.0    0.0  2024     2  29      0.518072    0.500000   \n",
      "\n",
      "  is_weekend  \n",
      "0        0.0  \n",
      "1        0.0  \n",
      "2        0.0  \n",
      "3        0.0  \n",
      "4        1.0  \n",
      "5        1.0  \n",
      "6        0.0  \n",
      "7        0.0  \n",
      "8        0.0  \n",
      "9        0.0  \n",
      "\n",
      "[10 rows x 56 columns]\n"
     ]
    }
   ],
   "source": [
    "to_normalize = ['sleepBPM', 'sleepBPMAvg7', 'dayBPM', 'dayBPMAvg7', 'wakingBPM', 'wakingBPMAvg7', \n",
    "            'hrv', 'hrvAvg7', 'sleepHRV', 'sleepHRVAvg7', 'SpO2Avg', 'SpO2Min', 'SpO2Max', \n",
    "            'respAvg', 'respMin', 'respMax', 'inBed_minutes', 'day_of_week', 'is_weekend', \n",
    "            'first_alarm', 'num_alarms', 'sleep_medicine', 'waking_temp', 'intoxicated', \n",
    "            'sick', 'eat_before_bed', 'sleep_alone', 'own_bed', 'stress', 'phone_before_bed', \n",
    "            'caffeine_before_bed']\n",
    "\n",
    "\n",
    "for cat in to_normalize:\n",
    "    merged_data[cat] = merged_data[cat] / (merged_data[cat].max())\n",
    "\n",
    "\n",
    "merged_data.dropna(axis=1, inplace=True)\n",
    "print(merged_data.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:22\n"
     ]
    }
   ],
   "source": [
    "features = ['sleepBPM', \n",
    "            # 'sleepBPMAvg7', \n",
    "            # 'dayBPM', \n",
    "            # 'dayBPMAvg7',\n",
    "              'wakingBPM',\n",
    "            #    'wakingBPMAvg7', \n",
    "            # 'hrv',\n",
    "            #   'hrvAvg7', \n",
    "            #   'sleepHRV', \n",
    "            #   'sleepHRVAvg7', \n",
    "            #   'SpO2Avg', \n",
    "            #   'SpO2Min', 'SpO2Max', \n",
    "            # 'respAvg', 'respMin', 'respMax', \n",
    "            # 'year',\n",
    "            #  'month', \n",
    "            # 'day', \n",
    "            'inBed_minutes', \n",
    "            # 'day_of_week', \n",
    "            'is_weekend',\n",
    "              ]\n",
    "features += [\n",
    "# 'first_alarm', \n",
    "'num_alarms', \n",
    "'sleep_medicine', \n",
    "# 'waking_temp', \n",
    "# 'intoxicated', \n",
    "# 'sick', \n",
    "# 'eat_before_bed', \n",
    "# 'sleep_alone',\n",
    "#  'own_bed', \n",
    "# 'stress', \n",
    "# 'phone_before_bed', \n",
    "# 'caffeine_before_bed',\n",
    "#  'grogginess_lag1'\n",
    " ]\n",
    "\n",
    "target = ['grogginess']\n",
    "# merged_data[features].head().to_csv('out.csv')\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "features = [f for f in features if f in merged_data.columns]\n",
    "X = merged_data[features]  # Features\n",
    "y = merged_data[target]  # Assuming you have a target variable\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# split_point = int(len(X) * 0.9)\n",
    "split_point = len(X) - 35\n",
    "print(f'0:{split_point - 1}')\n",
    "X_train, X_test = X[:split_point], X[split_point:]\n",
    "y_train, y_test = y[:split_point], y[split_point:]\n",
    "\n",
    "# X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "  torch.cuda.manual_seed_all(42)\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# torch.cuda.set_device(torch.cuda.current_device())\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "batch_size = 32\n",
    "\n",
    "training_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_data = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(dataset=training_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GrogginessModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(GrogginessModel, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size) \n",
    "        self.layer2 = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "\n",
    "input_size = len(features) \n",
    "hidden_size = 5  \n",
    "output_size = 1\n",
    "model = GrogginessModel(input_size, hidden_size, output_size)\n",
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001) \n",
    "lambda1 = 0.01\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, criterion):\n",
    "  model.eval()  # Set the model to evaluation mode\n",
    "  total_loss = 0.0\n",
    "  with torch.no_grad():  # No need to track gradients\n",
    "    for inputs, targets in test_loader:\n",
    "      outputs = model(inputs)\n",
    "      loss = criterion(outputs, targets)\n",
    "      l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
    "      total_loss += loss + lambda1 * l1_norm\n",
    "      \n",
    "  avg_loss = total_loss / len(test_loader.dataset)\n",
    "  return avg_loss\n",
    "  \n",
    "class EarlyStopping:\n",
    "  def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt'):\n",
    "    self.patience = patience\n",
    "    self.verbose = verbose\n",
    "    self.counter = 0\n",
    "    self.best_score = None\n",
    "    self.early_stop = False\n",
    "    self.val_loss_min = np.Inf\n",
    "    self.delta = delta\n",
    "    self.path = path\n",
    "\n",
    "  def __call__(self, val_loss, model):\n",
    "    score = -val_loss\n",
    "\n",
    "    if self.best_score is None:\n",
    "      self.best_score = score\n",
    "      self.save_checkpoint(val_loss, model)\n",
    "    elif score < self.best_score + self.delta:\n",
    "      self.counter += 1\n",
    "      # print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "      if self.counter >= self.patience:\n",
    "          self.early_stop = True\n",
    "    else:\n",
    "      self.best_score = score\n",
    "      self.save_checkpoint(val_loss, model)\n",
    "      self.counter = 0\n",
    "\n",
    "  def save_checkpoint(self, val_loss, model):\n",
    "    if self.verbose:\n",
    "      print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "    torch.save(model.state_dict(), self.path)\n",
    "    self.val_loss_min = val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000, Loss: 27.0945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1001/10000, Loss: 5.8310\n",
      "Epoch 2001/10000, Loss: 5.3129\n",
      "Epoch 3001/10000, Loss: 4.8999\n",
      "Epoch 4001/10000, Loss: 4.4751\n",
      "Epoch 5001/10000, Loss: 3.9545\n",
      "Epoch 6001/10000, Loss: 3.4562\n",
      "Epoch 7001/10000, Loss: 3.3293\n",
      "Epoch 8001/10000, Loss: 3.3004\n",
      "Epoch 9001/10000, Loss: 3.2810\n"
     ]
    }
   ],
   "source": [
    "epochs = 10000\n",
    "early_stopping = EarlyStopping(patience=1000, verbose=False)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    val_loss = evaluate_model(model, test_loader, criterion)\n",
    "    early_stopping(val_loss, model)\n",
    "    running_loss = 0.0\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        outputs = model(inputs)  # Forward pass\n",
    "        loss = criterion(outputs, targets)  # Compute loss\n",
    "        l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
    "        total_loss = loss + lambda1 * l1_norm\n",
    "        total_loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Optimize\n",
    "        \n",
    "        running_loss += total_loss.item() * inputs.size(0)  # Total loss for the batch\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)  # Average loss for the epoch\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sleepBPM', 'wakingBPM', 'inBed_minutes', 'is_weekend', 'num_alarms', 'sleep_medicine']\n"
     ]
    }
   ],
   "source": [
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sleepBPM          0.719154\n",
      "wakingBPM         0.808219\n",
      "inBed_minutes     0.590361\n",
      "is_weekend        0.000000\n",
      "num_alarms        1.000000\n",
      "sleep_medicine    0.000000\n",
      "Name: 23, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "realtime_data_values = X_test[features].iloc[0]\n",
    "print(realtime_data_values)\n",
    "realtime_data_values = realtime_data_values.tolist()\n",
    "realtime_data_tensor = torch.tensor([realtime_data_values], dtype=torch.float32)  # Wrap in a list to keep dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test loss: 0.2607\n"
     ]
    }
   ],
   "source": [
    "avg_test_loss = evaluate_model(model, test_loader, criterion)\n",
    "print(f'Average test loss: {avg_test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test loss: 2.8817\n",
      "Score (Sum of Absolute Differences): 46.91\n",
      "Average Score Per Day: 1.34 for 35 days\n"
     ]
    }
   ],
   "source": [
    "model = GrogginessModel(input_size, hidden_size, output_size)\n",
    "model.load_state_dict(torch.load('checkpoint.pt'))\n",
    "model.eval()\n",
    "\n",
    "score = 0\n",
    "\n",
    "total_loss = 0.0\n",
    "\n",
    "for inputs, targets in test_loader:\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "    total_loss += loss.item() * inputs.size(0)\n",
    "    score += torch.sum(torch.abs(outputs - targets)).item()\n",
    "\n",
    "\n",
    "avg_loss = total_loss / len(X_test_tensor)\n",
    "print(f'Average test loss: {avg_loss:.4f}')\n",
    "print(f'Score (Sum of Absolute Differences): {score:.2f}')\n",
    "print(f'Average Score Per Day: {score / len(X_test_tensor):.2f} for {len(X_test_tensor)} days')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
